{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari_Wrapper(gym.Wrapper):\n",
    "    # env wrapper to resize images, grey scale and frame stacking and other misc.\n",
    "    \n",
    "    def __init__(self, env, env_name, k, dsize=(84,84), use_add_done=False):\n",
    "        super(Atari_Wrapper, self).__init__(env)\n",
    "        self.dsize = dsize\n",
    "        self.k = k\n",
    "        self.use_add_done = use_add_done\n",
    "        \n",
    "        # set image cutout depending on game\n",
    "        if \"Pong\" in env_name:\n",
    "            self.frame_cutout_h = (33,-15)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        elif \"Breakout\" in env_name:\n",
    "            self.frame_cutout_h = (31,-16)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        elif \"SpaceInvaders\" in env_name:\n",
    "            self.frame_cutout_h = (25,-7)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        else:\n",
    "            # no cutout\n",
    "            self.frame_cutout_h = (0,-1)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        \n",
    "    def reset(self):\n",
    "    \n",
    "        self.Return = 0\n",
    "        self.last_life_count = 0\n",
    "        \n",
    "        ob = self.env.reset()\n",
    "        ob = self.preprocess_observation(ob)\n",
    "        \n",
    "        # stack k times the reset ob\n",
    "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
    "        \n",
    "        return self.frame_stack\n",
    "    \n",
    "    \n",
    "    def step(self, action): \n",
    "        # do k frameskips, same action for every intermediate frame\n",
    "        # stacking k frames\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        additional_done = False\n",
    "        \n",
    "        # k frame skips or end of episode\n",
    "        frames = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ob, r, d, info = self.env.step(action)\n",
    "            \n",
    "            # insert a (additional) done, when agent loses a life (Games with lives)\n",
    "            if self.use_add_done:\n",
    "                if info['ale.lives'] < self.last_life_count:\n",
    "                    additional_done = True  \n",
    "                self.last_life_count = info['ale.lives']\n",
    "            \n",
    "            ob = self.preprocess_observation(ob)\n",
    "            frames.append(ob)\n",
    "            \n",
    "            # add reward\n",
    "            reward += r\n",
    "            \n",
    "            if d: # env done\n",
    "                done = True\n",
    "                break\n",
    "                       \n",
    "        # build the observation\n",
    "        self.step_frame_stack(frames)\n",
    "        \n",
    "        # add info, get return of the completed episode\n",
    "        self.Return += reward\n",
    "        if done:\n",
    "            info[\"return\"] = self.Return\n",
    "            \n",
    "        # clip reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return self.frame_stack, reward, done, info, additional_done\n",
    "    \n",
    "    def step_frame_stack(self, frames):\n",
    "        \n",
    "        num_frames = len(frames)\n",
    "        \n",
    "        if num_frames == self.k:\n",
    "            self.frame_stack = np.stack(frames)\n",
    "        elif num_frames > self.k:\n",
    "            self.frame_stack = np.array(frames[-k::])\n",
    "        else: # mostly used when episode ends \n",
    "            \n",
    "            # shift the existing frames in the framestack to the front=0 (0->k, index is time)\n",
    "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
    "            # insert the new frames into the stack\n",
    "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
    "            \n",
    "    def preprocess_observation(self, ob):\n",
    "    # resize and grey and cutout image\n",
    "    \n",
    "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n",
    "                           self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
    "        ob = cv2.resize(ob, dsize=self.dsize)\n",
    "    \n",
    "        return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # nature paper architecture\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ]\n",
    "        \n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions, epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = DQN(in_channels, num_actions)\n",
    "        \n",
    "        self.eps = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "    \n",
    "    def e_greedy(self, x):\n",
    "        \n",
    "        actions = self.forward(x)\n",
    "        \n",
    "        greedy = torch.rand(1)\n",
    "        if self.eps < greedy:\n",
    "            return torch.argmax(actions)\n",
    "        else:\n",
    "            return (torch.rand(1) * self.num_actions).type('torch.LongTensor')[0] \n",
    "        \n",
    "    def greedy(self, x):\n",
    "        actions = self.forward(x)\n",
    "        return torch.argmax(actions)\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.eps = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "        f = open(f\"{self.filename}.csv\", \"w\")\n",
    "        f.close()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()\n",
    "\n",
    "class Experience_Replay():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transitions):\n",
    "        \n",
    "        for i in range(len(transitions)):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = transitions[i]\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        #return random.sample(self.memory, batch_size)\n",
    "        indexes = (np.random.rand(batch_size) * (len(self.memory)-1)).astype(int)\n",
    "        return [self.memory[i] for i in indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class Env_Runner:\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.logger = Logger(\"training_info\")\n",
    "        self.logger.log(\"training_step, return\")\n",
    "        \n",
    "        self.ob = self.env.reset()\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def run(self, steps):\n",
    "        \n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            self.ob = torch.tensor(self.ob) # uint8\n",
    "            action = self.agent.e_greedy(\n",
    "                self.ob.to(device).to(dtype).unsqueeze(0) / 255) # float32+norm\n",
    "            action = action.detach().cpu().numpy()\n",
    "            \n",
    "            obs.append(self.ob)\n",
    "            actions.append(action)\n",
    "            \n",
    "            self.ob, r, done, info, additional_done = self.env.step(action)\n",
    "               \n",
    "            if done: # real environment reset, other add_dones are for q learning purposes\n",
    "                self.ob = self.env.reset()\n",
    "                if \"return\" in info:\n",
    "                    self.logger.log(f'{self.total_steps+step},{info[\"return\"]}')\n",
    "            \n",
    "            rewards.append(r)\n",
    "            dones.append(done or additional_done)\n",
    "            \n",
    "        self.total_steps += steps\n",
    "                                    \n",
    "        return obs, actions, rewards, dones\n",
    "    \n",
    "def make_transitions(obs, actions, rewards, dones):\n",
    "    # observations are in uint8 format\n",
    "    \n",
    "    tuples = []\n",
    "\n",
    "    steps = len(obs) - 1\n",
    "    for t in range(steps):\n",
    "        tuples.append((obs[t],\n",
    "                       actions[t],\n",
    "                       rewards[t],\n",
    "                       obs[t+1],\n",
    "                       int(not dones[t])))\n",
    "        \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#env_name = 'BreakoutNoFrameskip-v4'\n",
    "#env_name = 'PongNoFrameskip-v4'\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "\n",
    "# hyperparameter\n",
    "\n",
    "num_stacked_frames = 4\n",
    "\n",
    "replay_memory_size = 250000\n",
    "min_replay_size_to_update = 25000\n",
    "\n",
    "lr = 6e-5 # SpaceInvaders #1e-4 for PONG | 2.5e-5 for Breakout\n",
    "gamma = 0.99\n",
    "minibatch_size = 32\n",
    "steps_rollout = 16\n",
    "\n",
    "start_eps = 1\n",
    "final_eps = 0.1\n",
    "\n",
    "final_eps_frame = 1000000\n",
    "total_steps = 20000000\n",
    "\n",
    "target_net_update = 625 # 10000 steps\n",
    "\n",
    "save_model_steps = 500000\n",
    "\n",
    "# init\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames, use_add_done=True)\n",
    "\n",
    "in_channels = num_stacked_frames\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "eps_interval = start_eps-final_eps\n",
    "\n",
    "agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "replay = Experience_Replay(replay_memory_size)\n",
    "runner = Env_Runner(env, agent)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr) #optim.RMSprop(agent.parameters(), lr=lr)\n",
    "huber_loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "num_steps = 0\n",
    "num_model_updates = 0\n",
    "\n",
    "start_time = time.time()\n",
    "while num_steps < total_steps:\n",
    "    \n",
    "    # set agent exploration | cap exploration after x timesteps to final epsilon\n",
    "    new_epsilon = np.maximum(final_eps, start_eps - ( eps_interval * num_steps/final_eps_frame))\n",
    "    agent.set_epsilon(new_epsilon)\n",
    "    \n",
    "    # get data\n",
    "    obs, actions, rewards, dones = runner.run(steps_rollout)\n",
    "    transitions = make_transitions(obs, actions, rewards, dones)\n",
    "    replay.insert(transitions)\n",
    "    \n",
    "    # add\n",
    "    num_steps += steps_rollout\n",
    "    \n",
    "    # check if update\n",
    "    if num_steps < min_replay_size_to_update:\n",
    "        continue\n",
    "    \n",
    "    # update\n",
    "    for update in range(4):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        minibatch = replay.get(minibatch_size)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        obs = (torch.stack([i[0] for i in minibatch]).to(device).to(dtype)) / 255 \n",
    "        \n",
    "        actions = np.stack([i[1] for i in minibatch])\n",
    "        rewards = torch.tensor([i[2] for i in minibatch]).to(device)\n",
    "        \n",
    "        # uint8 to float32 and normalize to 0-1\n",
    "        next_obs = (torch.stack([i[3] for i in minibatch]).to(device).to(dtype)) / 255\n",
    "        \n",
    "        dones = torch.tensor([i[4] for i in minibatch]).to(device)\n",
    "        \n",
    "        #  *** double dqn ***\n",
    "        # prediction\n",
    "        \n",
    "        Qs = agent(torch.cat([obs, next_obs]))\n",
    "        obs_Q, next_obs_Q = torch.split(Qs, minibatch_size ,dim=0)\n",
    "        \n",
    "        obs_Q = obs_Q[range(minibatch_size), actions]\n",
    "        \n",
    "        # target\n",
    "        \n",
    "        next_obs_Q_max = torch.max(next_obs_Q,1)[1].detach()\n",
    "        target_Q = target_agent(next_obs)[range(minibatch_size), next_obs_Q_max].detach()\n",
    "        \n",
    "        target = rewards + gamma * target_Q * dones\n",
    "        \n",
    "        # loss\n",
    "        loss = huber_loss(obs_Q, target) # torch.mean(torch.pow(obs_Q - target, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    num_model_updates += 1\n",
    "     \n",
    "    # update target network\n",
    "    if num_model_updates%target_net_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "    \n",
    "    # print time\n",
    "    if num_steps%50000 < steps_rollout:\n",
    "        end_time = time.time()\n",
    "        print(f'*** total steps: {num_steps} | time(50K): {end_time - start_time} ***')\n",
    "        start_time = time.time()\n",
    "    \n",
    "    # save the dqn after some time\n",
    "    if num_steps%save_model_steps < steps_rollout:\n",
    "        torch.save(agent,f\"{env_name}-{num_steps}.pt\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch\n",
    "\n",
    "# save agent\n",
    "torch.save(agent,\"agent.pt\")\n",
    "# load agent\n",
    "agent = torch.load(\"agent.pt\")\n",
    "\n",
    "#env = gym.make(env_name)\n",
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames)\n",
    "\n",
    "steps = 5000\n",
    "ob = env.reset()\n",
    "agent.set_epsilon(0.025)\n",
    "agent.eval()\n",
    "imgs = []\n",
    "for step in range(steps):\n",
    "            \n",
    "    action = agent.e_greedy(torch.tensor(ob, dtype=dtype).unsqueeze(0).to(device) / 255)\n",
    "    action = action.detach().cpu().numpy()\n",
    "    #action = env.action_space.sample()\n",
    "\n",
    "    env.render()\n",
    "    ob, _, done, info, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(0.016)        \n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        print(info)\n",
    "    \n",
    "    imgs.append(ob)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
