{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#env_name = 'BreakoutNoFrameskip-v4'\n",
    "#env_name = 'PongNoFrameskip-v4'\n",
    "env_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "\n",
    "total_episodes = 14\n",
    "eval_epsilon = 0.01\n",
    "num_stacked_frames = 4\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari_Wrapper(gym.Wrapper):\n",
    "    # env wrapper to resize images, grey scale and frame stacking and other misc.\n",
    "    \n",
    "    def __init__(self, env, env_name, k, dsize=(84,84), use_add_done=False):\n",
    "        super(Atari_Wrapper, self).__init__(env)\n",
    "        self.dsize = dsize\n",
    "        self.k = k\n",
    "        self.use_add_done = use_add_done\n",
    "        \n",
    "        # set image cutout depending on game\n",
    "        if \"Pong\" in env_name:\n",
    "            self.frame_cutout_h = (33,-15)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        elif \"Breakout\" in env_name:\n",
    "            self.frame_cutout_h = (31,-16)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        elif \"SpaceInvaders\" in env_name:\n",
    "            self.frame_cutout_h = (25,-7)\n",
    "            self.frame_cutout_w = (7,-7)\n",
    "        else:\n",
    "            # no cutout\n",
    "            self.frame_cutout_h = (0,-1)\n",
    "            self.frame_cutout_w = (0,-1)\n",
    "        \n",
    "    def reset(self):\n",
    "    \n",
    "        self.Return = 0\n",
    "        self.last_life_count = 0\n",
    "        \n",
    "        ob = self.env.reset()\n",
    "        ob = self.preprocess_observation(ob)\n",
    "        \n",
    "        # stack k times the reset ob\n",
    "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
    "        \n",
    "        return self.frame_stack\n",
    "    \n",
    "    \n",
    "    def step(self, action, render=False): \n",
    "        # do k frameskips, same action for every intermediate frame\n",
    "        # stacking k frames\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        additional_done = False\n",
    "        \n",
    "        # k frame skips or end of episode\n",
    "        frames = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ob, r, d, info = self.env.step(action)\n",
    "            if render:\n",
    "                self.render()\n",
    "                time.sleep(0.004)\n",
    "            \n",
    "            # insert a (additional) done, when agent loses a life (Games with lives)\n",
    "            if self.use_add_done:\n",
    "                if info['ale.lives'] < self.last_life_count:\n",
    "                    additional_done = True  \n",
    "                self.last_life_count = info['ale.lives']\n",
    "            \n",
    "            ob = self.preprocess_observation(ob)\n",
    "            frames.append(ob)\n",
    "            \n",
    "            # add reward\n",
    "            reward += r\n",
    "            \n",
    "            if d: # env done\n",
    "                done = True\n",
    "                break\n",
    "                       \n",
    "        # build the observation\n",
    "        self.step_frame_stack(frames)\n",
    "        \n",
    "        # add info, get return of the completed episode\n",
    "        self.Return += reward\n",
    "        if done:\n",
    "            info[\"return\"] = self.Return\n",
    "            \n",
    "        # clip reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return self.frame_stack, reward, done, info, additional_done\n",
    "    \n",
    "    def step_frame_stack(self, frames):\n",
    "        \n",
    "        num_frames = len(frames)\n",
    "        \n",
    "        if num_frames == self.k:\n",
    "            self.frame_stack = np.stack(frames)\n",
    "        elif num_frames > self.k:\n",
    "            self.frame_stack = np.array(frames[-k::])\n",
    "        else: # mostly used when episode ends \n",
    "            \n",
    "            # shift the existing frames in the framestack to the front=0 (0->k, index is time)\n",
    "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
    "            # insert the new frames into the stack\n",
    "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
    "            \n",
    "    def preprocess_observation(self, ob):\n",
    "    # resize and grey and cutout image\n",
    "    \n",
    "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],\n",
    "                           self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
    "        ob = cv2.resize(ob, dsize=self.dsize)\n",
    "    \n",
    "        return ob\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    # nature paper architecture\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ]\n",
    "        \n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions, epsilon):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = DQN(in_channels, num_actions)\n",
    "        \n",
    "        self.eps = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "    \n",
    "    def e_greedy(self, x):\n",
    "        \n",
    "        actions = self.forward(x)\n",
    "        \n",
    "        greedy = torch.rand(1)\n",
    "        if self.eps < greedy:\n",
    "            return torch.argmax(actions)\n",
    "        else:\n",
    "            return (torch.rand(1) * self.num_actions).type('torch.LongTensor')[0] \n",
    "        \n",
    "    def greedy(self, x):\n",
    "        actions = self.forward(x)\n",
    "        return torch.argmax(actions)\n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.eps = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(env_name+\"-Eval\"+\".csv\",\"w\")\n",
    "\n",
    "for filename in os.listdir():\n",
    "\n",
    "    if env_name not in filename or \".pt\" not in filename:\n",
    "        continue\n",
    "    \n",
    "    print(\"load file name\", filename)\n",
    "    agent = torch.load(filename).to(device)\n",
    "    agent.set_epsilon(eval_epsilon)\n",
    "    agent.eval()\n",
    "\n",
    "    raw_env = gym.make(env_name)\n",
    "    env = Atari_Wrapper(raw_env, env_name, num_stacked_frames)\n",
    "    \n",
    "    \n",
    "\n",
    "    ob = env.reset()\n",
    "    num_episode = 0\n",
    "    returns = []\n",
    "    while num_episode < total_episodes:\n",
    "\n",
    "        action = agent.e_greedy(torch.tensor(ob, dtype=dtype).unsqueeze(0).to(device) / 255)\n",
    "        action = action.detach().cpu().numpy()\n",
    "\n",
    "        ob, _, done, info, _ = env.step(action, render=True)\n",
    "\n",
    "        #time.sleep(0.016)  \n",
    "\n",
    "        if done:\n",
    "            ob = env.reset()\n",
    "            returns.append(info[\"return\"])\n",
    "            num_episode += 1\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    steps = filename.strip().split(\".\")[0].split(\"-\")[-1]\n",
    "    f.write(f'{steps},{np.mean(returns)}\\n')\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
